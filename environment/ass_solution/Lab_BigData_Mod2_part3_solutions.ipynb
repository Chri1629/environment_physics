{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# LAB MODULE 2.\n",
    "# Observational data in climate science\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# Topics covered:\n",
    "# (1) Organization of geospatial data in python\n",
    "# (2) Operations in time\n",
    "# (3) Operations in space \n",
    "\n",
    "# The purpose of this exercise is to (partially) replicate the process of \n",
    "# building of a global mean tempertaure anomaly curve from observations \n",
    "\n",
    "# Links: \n",
    "#   https://journals.ametsoc.org/doi/pdf/10.1175/1520-0450%281986%29025%3C0161%3ANHSATV%3E2.0.CO%3B2\n",
    "#   https://www.metoffice.gov.uk/hadobs/crutem4/data/download.html  >>>  CRUTEM.4.6.0.0.station_files.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import os\n",
    "\n",
    "# A bunch of other libraries that may be useful (you can use them or ignore them, or import different ones)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import datetime as dtm\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's have a look at one of them\n",
    "# You can open the text files direclty with a text editor \n",
    "# Check as well the template with explanations: \"crutem4_data_station_file_format.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "#-- get info on database and files organization --\n",
    "#-------------------------------------------------\n",
    "\n",
    "flist = [os.path.join(path, name) for path, subdirs, files in os.walk(\"./CRUTEM.4.6.0.0.station_files/\") for name in files]\n",
    "for i in range(0,5):\n",
    "  print (flist[i])\n",
    "\n",
    "# exclude first 2 items, not relevant\n",
    "flist=flist[2:]\n",
    "\n",
    "# Alternativly\n",
    "#import glob\n",
    "#flist = glob.glob(\"./CRUTEM.4.6.0.0.station_files/\"+'/*/*')\n",
    "\n",
    "print ('\\n',flist[0:5])\n",
    "nst=len(flist)\n",
    "print (\"\\n > Number of stations = \",nst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 \n",
    "# We need to import the data in python and organize them for further analyses and operations, both in time and space.\n",
    "\n",
    "# Try to start thinking of a suitable way to organize this dataset, consisting of ~10,000 stations, each one having:\n",
    "# - a unique identifier\n",
    "# - string metadata (e.g. name, country)\n",
    "# - numerical metadata (e.g. latitude, longitude, elevation) >> lat & lon are also the spatial dimensions we'll need\n",
    "# - other metadata\n",
    "# - a time series at monthly resolution oragnized as 2D tabs (year, month)\n",
    "\n",
    "# Consider the data structures you know of, keeping in mind the final purpose that will require somehow \n",
    "# aggregating this dataset in time and space. \n",
    "\n",
    "# Bear in mind that this is real raw data that could have incomplete/erroneous file coding etc.\n",
    "# Other than that, we will assume that all the reported data are correct and directly usable, \n",
    "#   considering the reality that time series have missing data and different time span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will consider the actual time & space operation in the next stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are back !\n",
    "# Here are a few initial considerations.\n",
    "\n",
    "# First, let's start from the consideration that time series have missing data and different time span.\n",
    "# In order to oraganize temporal data coherently, we need a common time axis, on which to place correctly our obs.\n",
    "# >> datetime objects provide this kind of functionality\n",
    "\n",
    "# The time series are currently organized as inpependent 2D tabs; \n",
    "# it will be necessary to expand them into 1D time series\n",
    "\n",
    "# In this way we could associate a 1D time series to each station;\n",
    "# a pandas dataframe (which can host datetime objects) coust host this kind of data ...\n",
    "\n",
    "# ... however, we also have metadata to consider. Above all, we need lat & lon to place our station in space.\n",
    "# In addition, we have staion names, countries and elevation which may be of interest.\n",
    "# Additional metadata could be useful (or not?) for later data processing.\n",
    "\n",
    "# What about geopandas? \n",
    "# That could at least store lat/lon coordinates as a point geometry.\n",
    "# True. However, other metadata do not find a place in this configuration.\n",
    "\n",
    "# Could we place all metadata for a station at the same level of all the tempearture values?\n",
    "# We could, at the price of losing the datetime functionalities ... does not seemm worth\n",
    "\n",
    "# Is there any n-D structure that could host our data?\n",
    "# numpy arrays indeed can be up to 7D, but they do not seem to be what we need here.\n",
    "\n",
    "# MORALE: it is challenging to find a unique data structure to accommodate conveniently the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible strategies >>\n",
    "\n",
    "# (A) Store temperature time series per each station in pandas df \n",
    "#       (rows=stations & columns=months; or the other way round ?)\n",
    "#     and all the metadata in a separate df. A unique identifier will allow relating the 2 dataframes at any time.\n",
    "\n",
    "# (B) Same as above, except using geopandas gdf to also include lat/lon along with data, rather than with metadata.\n",
    "#     This option may be useful if we ever needed gdp functions for space operations. \n",
    "#     BUT: I can only place geometry in a column, which means they will be associated to a row representing a station.\n",
    "#     What about datetime objects? Could they stay in columns as well ? \n",
    "#     Or they need to be asscoated to the df index (so to be the 1st column themselves) to be used as we need ?\n",
    "\n",
    "# (C) Define a new class of objects including the structures defined above.\n",
    "#     This would \"free\" us from having to keep track of two differnt dataframes, with more compact and efficient code,\n",
    "#     but it likely implies more effort to define the class and procedures.\n",
    "\n",
    "# (D) Other ideas ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example I will start by simply acquiring data & metadata in separate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first challenge is to correctly read in the data from an individual ASCII files.\n",
    "\n",
    "# Define df to store metadata.\n",
    "# Station IDs will be used as unique identifiers for both dataframes.\n",
    "\n",
    "metadata0 = pd.DataFrame(columns=['ID','stname','country','elev','lat','lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I will define a common time axis, along which all time series will be aligned. This will be the index of gdf.\n",
    "\n",
    "taxis = pd.date_range('1850-01', '2019-01', freq='M')\n",
    "nmonths=len(taxis)\n",
    "nyears=nmonths/12\n",
    "\n",
    "# Define df to store data (temperature time series) and location. \n",
    "\n",
    "data0 = pd.DataFrame({'time': taxis})\n",
    "data0 = data0.set_index(['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reference period\n",
    "yref0 = 1961\n",
    "yref1 = 1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # FROM THE TEMPLATE ...    \n",
    "\n",
    "  # Number= 037760  \n",
    "  # Name= LONDON/GATWICK\n",
    "  # Country= UK\n",
    "  # Lat=   51.2\n",
    "  # Long=    0.2\n",
    "  # Height= 59\n",
    "  # ...\n",
    "  # Obs:\n",
    "  # 1961   3.9   7.1   7.5  10.3  11.0  14.7  15.9  16.1  15.7  10.9   6.4   2.7   501   501   501   501   501   501   501   501   501   501   501   501 \n",
    "  # 1962   4.1   4.5   2.5   7.6   9.8  13.0  15.1  14.7  12.6  10.5   5.9   1.7   501   501   501   501   501   501   501   501   501   501   501   501 \n",
    "  # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset acquisition\n",
    "\n",
    "nst_tmp = nst   #nst #200\n",
    "\n",
    "#- set counters for diagnostics \n",
    "nodatacount=0  \n",
    "tooshortcount=0 \n",
    "outsidecount=0\n",
    "\n",
    "from tqdm import tqdm\n",
    "for si in tqdm(range(0,nst_tmp)):\n",
    "\n",
    "  #- open file \n",
    "  \n",
    "  filein=flist[si]\n",
    "  \n",
    "  #- Scan text file to find starting line of data matrix &\n",
    "  #- check for the actual presence of data\n",
    "\n",
    "  with open(filein) as f: data = f.readlines()\n",
    "  skipi = data.index(\"Obs:\\n\")+1\n",
    "  if (len(data)-skipi < 2):\n",
    "    nodatacount+=1    \n",
    "    continue\n",
    "\n",
    "  #- Find out how many years are in the time series for this station &\n",
    "  #-   apply some filters on the time series lenght and/or timing as a condition to include the station in the dataset\n",
    "  #- I could use the metadata (Start year, End year) or \n",
    "  #-   get this piece of info from the actual data >>> I chose the second option   \n",
    "    \n",
    "  #- I read the first column of the data table, which constains the list of years\n",
    "  yr = np.genfromtxt(filein, skip_header=skipi, delimiter=None, usecols=0, dtype='i4')\n",
    "  nyr = len(yr)\n",
    "\n",
    "  # this filters out data series that are too short (e.g. less than 30 years)  \n",
    "  if (nyr < 30):\n",
    "    tooshortcount+=1 \n",
    "    continue  \n",
    "\n",
    "  # do not retain series not covering at least in part the reference period   \n",
    "  #if (yr[0]>yref1 or yr[-1]<yref0):\n",
    "  #  outsidecount+=1 \n",
    "  #  continue       \n",
    "\n",
    "  # OR  \n",
    "    \n",
    "  # retain only the stations for which the calculation of the average annual cycle for the reference period \n",
    "  # yields non-NAN values for each month, i.e. the normals have no missing value\n",
    "    \n",
    "  norm_line = [line for line in data if \"Normals=\" in line]\n",
    "  norm = str(norm_line).split(\"=\",1)[1]\n",
    "  norm  = norm[:-4].strip() \n",
    "  if (norm.find(\"-99.0\") > -1):   #str.find returns the position of the given substring, -1 if not found\n",
    "    outsidecount+=1 \n",
    "    continue   \n",
    "        \n",
    "  #- If the station is retained, then gather data & metadata \n",
    "\n",
    "  # FROM THE TEMPLATE ...  \n",
    "  # Name= LONDON/GATWICK\n",
    "  # Country= UK\n",
    "  # Lat=   51.2\n",
    "  # Long=    0.2\n",
    "  # Height= 59\n",
    "\n",
    "  # Get local values for * METADATA * of interest\n",
    "\n",
    "  stname_line = [line for line in data if \"Name=\" in line]\n",
    "  stname = str(stname_line).split(\"=\",1)[1]\n",
    "  stname  = stname[:-4].strip()    \n",
    "    \n",
    "  country_line = [line for line in data if \"Country=\" in line]\n",
    "  country = str(country_line).split(\"=\",1)[1]\n",
    "  country  = country[:-4].strip()  # this removes leading & trailing strings, incl. \"new line\" [\\n] at the end \n",
    "\n",
    "  elev_line = [line for line in data if \"Height=\" in line]\n",
    "  elev = str(elev_line).split(\"=\",1)[1] \n",
    "  elev = float(elev[:-4].strip())\n",
    "\n",
    "  lat_line = [line for line in data if \"Lat=\" in line]\n",
    "  lat = str(lat_line).split(\"=\",1)[1] \n",
    "  lat = float(lat[:-4].strip())\n",
    "\n",
    "  lon_line = [line for line in data if \"Long=\" in line]\n",
    "  lon = str(lon_line).split(\"=\",1)[1] \n",
    "  lon = float(lon[:-4].strip())      \n",
    "\n",
    "  # Append local values to metadata df\n",
    "    \n",
    "  metadata0 = metadata0.append({\n",
    "     \"ID\": filein[-6:],\n",
    "     \"stname\": stname,\n",
    "     \"country\": country,\n",
    "     \"elev\": elev,\n",
    "     \"lat\":  lat,      \n",
    "     \"lon\":  lon,\n",
    "     }, ignore_index=True)\n",
    "    \n",
    "    \n",
    "  #- Finally, read and arrange * DATA * (temperature time series)\n",
    "\n",
    "  # FROM THE TEMPLATE ...  \n",
    "  # ...\n",
    "  # Obs:\n",
    "  # 1961   3.9   7.1   7.5  10.3  11.0  14.7  15.9  16.1  15.7  10.9   6.4   2.7   501   501   501   501   501   501   501   501   501   501   501   501 \n",
    "  # 1962   4.1   4.5   2.5   7.6   9.8  13.0  15.1  14.7  12.6  10.5   5.9   1.7   501   501   501   501   501   501   501   501   501   501   501   501 \n",
    "\n",
    "  # We need to go from 2D to 1D oragnization of temperature data &\n",
    "  #  associate each temperature to a datetime index\n",
    "  # This way we can coherently build a 2D df (time, station), \n",
    "  #  on which we could later drop stations or slice/gropuby by time  \n",
    "  # >>> I will build a local df, then append it to the general df  \n",
    "    \n",
    "  #- build local time axis\n",
    "  # yr: the first column of the data table, which constains the list of years (defined above)\n",
    "\n",
    "  ti0 = datetime.strptime(str(yr[0]), \"%Y\")\n",
    "  ti1 = datetime.strptime(str(yr[nyr-1]+1), \"%Y\")\n",
    "  stime = pd.date_range(ti0, ti1, freq='M')     \n",
    "  if ((yr[nyr-1]-yr[0]+1) != len(yr)):  # check if data of any year are missing within the first and last year\n",
    "    print('WARNING: station {} has missing years !'.format(metadata0.ID[si]))\n",
    "\n",
    "  #- read in the 2D table (left side with temperature data) as a numpy array, which I can later reshape into 1D\n",
    "\n",
    "  loc_data_2d = np.genfromtxt(filein, skip_header=skipi, filling_values='-99.0', delimiter=None, dtype='float')[:,1:13]  #[rows,cols]\n",
    "  loc_data_1d = loc_data_2d.flatten()\n",
    " \n",
    "  #- check that the order of element rearrangement is correct\n",
    "  #print(np.shape(loc_data_2d),nyr)\n",
    "  #print('@@',loc_data_2d[:2,:])\n",
    "  #print(loc_data_1d[:24],'\\n')\n",
    "\n",
    "  #- build local station dataframe, using the station ID as column name\n",
    "  \n",
    "  xdf = pd.DataFrame({'time': stime, filein[-6:]: np.asarray(loc_data_1d)})\n",
    "  xdf = xdf.set_index('time')\n",
    "  \n",
    "  #- merge into global dataframe, using the station ID as column name\n",
    "\n",
    "  data0 = pd.merge(data0, xdf, on='time', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some info\n",
    "\n",
    "print(nodatacount,' stations with no data (not acquired)')\n",
    "print(tooshortcount,' stations with too little data (not acquired)')\n",
    "print(outsidecount,' stations with not enough data for the ref. period (not acquired)')\n",
    "print(\"{} [{}] valid stations acquired\".format(nst_tmp-nodatacount-tooshortcount-outsidecount,len(metadata0.ID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data0.iloc[1500:1505,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now think of additional pre-processing operations / checks that might be desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all missing values (-99.0) into NAN\n",
    "data0[data0 == -99.0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take station IDs as index for metadata0\n",
    "metadata0 = metadata0.set_index(['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix longitudes\n",
    "metadata0.lon = -metadata0.lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stations in our dataset\n",
    "\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.stock_img()\n",
    "ax.scatter([metadata0.lon],[metadata0.lat],color='r',marker='o',s=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Plot time series from a few randomly chosen stations --\n",
    "\n",
    "#- Set multipanel plot\n",
    "fig = plt.figure(figsize=(13,19))\n",
    "\n",
    "no = 6   # number of randomn stations to pick\n",
    "\n",
    "subplots = (no,2)\n",
    "n_panels = subplots[0] * subplots[1]\n",
    "\n",
    "proj = ccrs.PlateCarree()\n",
    "\n",
    "#- Select sites\n",
    "rssite = np.random.randint(len(data0.columns), size=no)\n",
    "ssite = metadata0.index[rssite]\n",
    "\n",
    "for fi, f in enumerate(ssite):\n",
    "\n",
    "    #- locate on map\n",
    "    ax = fig.add_subplot(subplots[0], subplots[1], (fi*2)+1, projection=proj)\n",
    "    ax.set_title(' - '.join([metadata0.stname[ssite[fi]],metadata0.country[ssite[fi]]]))\n",
    "    ax.stock_img()\n",
    "    ax.coastlines()\n",
    "    plt.plot(metadata0.lon[ssite[fi]], metadata0.lat[ssite[fi]],\n",
    "         color='red', marker='o', markersize=5,\n",
    "         transform=ccrs.Geodetic())         \n",
    " \n",
    "    #- time series\n",
    "    tser = fig.add_subplot(subplots[0], subplots[1], (fi+1)*2)\n",
    "    #data0[ssite[fi]].plot()  \n",
    "    plt.plot(data0[ssite[fi]])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then let's start exploting our datasets towards the final goal\n",
    "# (1) Deal with the time series, in particular what do about missing data (nothing or anything else?)\n",
    "# (2) Transform all TEMPERATURE time series to TEMPERATURE ANOMALY time series, with respect to a reference period\n",
    "# (3) First step of spatial aggregation: gridding at monthly time step\n",
    "# (4) Aggregate the gridded data from monthly to annual time step\n",
    "# (5) Second step of spatial aggregation: derive the global time series from the gridded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Deal with the time series, in particular what to do about missing data \n",
    "\n",
    "# We could decide \n",
    "#   to retain only stations with complete records, \n",
    "#   or to retain for every station only the years with at least 6 months of data, etc.\n",
    "# We could also decide e.g.\n",
    "#   to assign the missing data based on the nearest (in time) value for the same month at the same station\n",
    "#   or to assign the missing data based on the nearest (in space) value for the same month from a different station\n",
    "\n",
    "# In this example I will just go on without further pre-processing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Transform all TEMPERATURE time series to TEMPERATURE ANOMALY time series, with respect to a reference period\n",
    "\n",
    "# ANOMALY(y,m) = DATA(y,m) - AVERAGE[ref. period](m) , for each month (m=1..12), for each year (y=1850..2019)\n",
    "\n",
    "# AVERAGE[ref. period](m) is in fact the \"Normals\", that we could have stored while acquiring the data & metadata, or\n",
    "#  alternatively we could recalculate here. I will show how to proceed for the second option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's identify the position along the time index that corresponds to the begining/end of the reference period\n",
    "\n",
    "x1=data0.index.get_loc('1961-01-31')\n",
    "x2=data0.index.get_loc('1990-12-31')\n",
    "\n",
    "# Then calculate the \"Normals\"\n",
    "\n",
    "data_normals = data0[x1:x2].groupby(data0[x1:x2].index.month).mean()\n",
    "print(data_normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's subtract normals from the the original time series, to get \n",
    "# the time series of the temperature anomalies for each station \n",
    "# (Note that this operation would be efficiently done with 2D data organization, such as in the original text files)\n",
    "\n",
    "# Make a copy of data0 (data_anom) to store the anomalies\n",
    "\n",
    "data_anom = data0\n",
    "\n",
    "# Then loop through all the years (January) in the time axis, and substract the Normals from each chunk of 12 months\n",
    "\n",
    "y0=0\n",
    "for yi in range(0,int(nyears)):\n",
    "  data_anom.iloc[y0:y0+12,:] = data0.iloc[y0:y0+12,:]-np.array(data_normals.iloc[:,:])\n",
    "  y0+=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Plot time series from a few randomly chosen stations --\n",
    "\n",
    "#- Set multipanel plot\n",
    "fig = plt.figure(figsize=(13,19))\n",
    "\n",
    "no = 6   # number of randomn stations to pick\n",
    "\n",
    "subplots = (no,2)\n",
    "n_panels = subplots[0] * subplots[1]\n",
    "\n",
    "proj = ccrs.PlateCarree()\n",
    "\n",
    "#- Select sites\n",
    "rssite = np.random.randint(len(data0.columns), size=no)\n",
    "ssite = metadata0.index[rssite]\n",
    "\n",
    "for fi, f in enumerate(ssite):\n",
    "\n",
    "    #- locate on map\n",
    "    ax = fig.add_subplot(subplots[0], subplots[1], (fi*2)+1, projection=proj)\n",
    "    ax.set_title(' - '.join([metadata0.stname[ssite[fi]],metadata0.country[ssite[fi]]]))\n",
    "    ax.stock_img()\n",
    "    ax.coastlines()\n",
    "    plt.plot(metadata0.lon[ssite[fi]], metadata0.lat[ssite[fi]],\n",
    "         color='red', marker='o', markersize=5,\n",
    "         transform=ccrs.Geodetic())         \n",
    " \n",
    "    #- time series\n",
    "    tser = fig.add_subplot(subplots[0], subplots[1], (fi+1)*2)\n",
    "    #data0[ssite[fi]].plot()  \n",
    "    plt.plot(data_anom[ssite[fi]])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) First step of spatial aggregation: gridding at monthly time step\n",
    "\n",
    "# All data are aggregated (averaged) in a regular lat/lon grid covering the Earth surface (e.g. 10x10 or 5x5 deg.)\n",
    "# In this way, we are \"homogenizing\" the spatial distribution of our dataset\n",
    "\n",
    "# Here there are a few strategies that can be followed. \n",
    "\n",
    "# One way would be to build a grid (e.g. a couple of 1D lists/arrays or one 2D array) containing the \n",
    "#  lats/lons margins of grid cells. Then slicing based on lats and lons ranges can be used to calculate the average\n",
    "#  time series from the grid cell.\n",
    "\n",
    "# A similar approach could be devised using geopandas, attributing lat/lon station coordinates to point geometries, \n",
    "#  defining the regular grid as a series of polygons. Then it would be possible to use geopandas geometry functions\n",
    "#  to operate a \"geometry slicing\", i.e. points contained within a given polygon.\n",
    "#  http://blog.yhat.com/posts/interactive-geospatial-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Aggregate the gridded data from monthly to annual time step\n",
    "\n",
    "# By doing this operation after (3), we are in fact still exploiting data from stations with some missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this phase we can either\n",
    "# - consider all data, irrespective of the fact that a given year has NAN for ceratin values\n",
    "# - or set to NAN all incomplete years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will proceed in this example, using the first strategy, for both (3) and (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid \n",
    "\n",
    "resol = 20 #5  # degrees (E and N)\n",
    "nlon = np.int(360/resol)\n",
    "nlat = np.int(180/resol)\n",
    "\n",
    "# define grid cell margins\n",
    "\n",
    "grlons = np.empty([nlon+1],dtype='float') \n",
    "grlats = np.empty([nlat+1],dtype='float') \n",
    "grlons[0] = -180.\n",
    "grlats[0] = -90.\n",
    "for i in range(1,nlon+1):\n",
    "  grlons[i]=grlons[i-1]+resol\n",
    "for i in range(1,nlat+1):\n",
    "  grlats[i]=grlats[i-1]+resol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data structure to host the gridded dataset:\n",
    "# what will it be ?  -> I chose 3D numpy array\n",
    "\n",
    "data_mo = np.empty([nmonths,nlat,nlon],dtype=float)\n",
    "data_mo[:,:,:] = np.nan\n",
    "\n",
    "data_yr = np.empty([int(nyears),nlat,nlon],dtype=float)\n",
    "data_yr[:,:,:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Iterate over grid cells and \n",
    "# - get the average time series of all stations in grid cell at monthy resolution\n",
    "# - integrate to annual resolutin  \n",
    "\n",
    "for j in range(0,nlat):\n",
    "  for i in range(0,nlon):\n",
    "    \n",
    "    dummy = metadata0[metadata0.lon >= grlons[i]]\n",
    "    dummy = dummy[dummy.lon < grlons[i+1]]\n",
    "    dummy = dummy[dummy.lat >= grlats[j]]\n",
    "    dummy = dummy[dummy.lat < grlats[j+1]]\n",
    "\n",
    "    #print('*',grlons[i],grlons[i+1],grlats[j],grlats[j+1])\n",
    "    #print(dummy.index)\n",
    "\n",
    "    if (len(dummy.index) > 0):\n",
    " \n",
    "      # calculation\n",
    "\n",
    "      data_mo[:,j,i] = np.array(data_anom[dummy.index].mean(axis=1)).flatten()\n",
    "        \n",
    "      data_yr[:,j,i] = np.array(data_anom[dummy.index].mean(axis=1).resample(\"Y\").mean()).flatten()\n",
    "\n",
    "      # display to check things  \n",
    "\n",
    "      fig = plt.figure(figsize=(13,3))  \n",
    "      subplots = (1,2)\n",
    "      n_panels = subplots[0] * subplots[1]\n",
    "\n",
    "      #- locate on map\n",
    "      ax = fig.add_subplot(subplots[0], subplots[1], 1, projection=ccrs.PlateCarree())\n",
    "      ax.stock_img()\n",
    "      ax.coastlines()\n",
    "      ax.gridlines(xlocs=range(-180,181,resol), ylocs=range(-90,91,resol))\n",
    "      ax.scatter([dummy.lon],[dummy.lat],color='r',marker='o',s=1.2)\n",
    "         \n",
    "      #- time series\n",
    "      tser = fig.add_subplot(subplots[0], subplots[1], 2)\n",
    "      plt.plot(taxis,data_anom[dummy.index],color='grey',linewidth=0.1)\n",
    "      plt.plot(taxis,data_mo[:,j,i],color='blue',linewidth=0.3)\n",
    "      plt.plot(pd.date_range('1850-01', '2019-01', freq='Y'),data_yr[:,j,i],color='red',linewidth=2)  \n",
    "\n",
    "      fig.tight_layout()  \n",
    "      plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Second step of spatial aggregation: derive the global time series from the gridded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have the options of a simple arithmetic average, or a weighted average\n",
    "# The weights could be on several levels, e.g.\n",
    "# - a measure of the density of observations or their coherence\n",
    "# - the unequal areas covered by each grid cell\n",
    "# - the unequal proportions of land vs ocean in different grid cells\n",
    "# - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, I will simply use the arithmetic average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to display to check things  \n",
    "\n",
    "fig = plt.figure(figsize=(13,13))  \n",
    "subplots = (2,1)\n",
    "\n",
    "ax = fig.add_subplot(subplots[0], subplots[1], 1, projection=ccrs.PlateCarree())\n",
    "ax.stock_img()\n",
    "ax.coastlines()\n",
    "ax.gridlines(xlocs=range(-180,181,resol), ylocs=range(-90,91,resol))\n",
    "\n",
    "tser = fig.add_subplot(subplots[0], subplots[1], 2)\n",
    "\n",
    "# Initialize data structure to host global average temperature anomaly time series\n",
    "\n",
    "data_glob = np.empty([int(nyears)],dtype=float)\n",
    "data_glob[:] = np.nan\n",
    "\n",
    "# loop over all grid cells\n",
    "\n",
    "for j in range(0,nlat):\n",
    "  for i in range(0,nlon):\n",
    "    \n",
    "    # Check if current grid cell has an associated time series\n",
    "    \n",
    "    if not(np.isnan(data_yr[:,j,i]).all()):\n",
    "     \n",
    "      #  place a marker on the map, and plot individual grid cell time series \n",
    "\n",
    "      ax.plot( grlons[i:i+2].sum()/2, grlats[j:j+2].sum()/2,\n",
    "         color='red', marker='x', markersize=5,transform=ccrs.PlateCarree())     \n",
    "      plt.plot(pd.date_range('1850-01', '2019-01', freq='Y'),data_yr[:,j,i],color='gray',linewidth=0.1) \n",
    "\n",
    "# calculate the global average at each time step      \n",
    "        \n",
    "for t in range(0,int(nyears)):  \n",
    "\n",
    "    local = data_yr[t,:,:]\n",
    "    valid = np.isnan(data_yr[t,:,:])\n",
    "    data_glob[t] = np.nanmean(local[~valid])\n",
    "\n",
    "# plot the global average curve  \n",
    "    \n",
    "plt.plot(pd.date_range('1850-01', '2019-01', freq='Y'),data_glob[:],color='red',linewidth=2) \n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to file the time series of global mean temperature anomalies\n",
    "\n",
    "gmta = pd.DataFrame({'time': pd.date_range('1850-01', '2019-01', freq='Y')})\n",
    "gmta['gmta'] = data_glob[:]\n",
    "gmta = gmta.set_index(['time'])\n",
    "\n",
    "gmta.to_csv('_'.join(['gmta',str(int(nst_tmp)),'stations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that estimation of uncertainties in the different passages could be calculated, and propagated\n",
    "# Also, it would be possible to perform several realizations of selecting sub-gropus of stations (bootstrap)\n",
    "# Several \"cleaning\" / pre-processing operations could be considered, e.g. removal of outliers (e.g. > n St.dev.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 2 (assignment)\n",
    "# Answer the following questions:\n",
    "# (1) What is the difference in the average the global mean temperature anomaly for the last 30 years of the record, compared to that of the first three decades of the twentieth century ?\n",
    "# (2) Is this difference statistically significant ?\n",
    "# You can use and extend the example from class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's slice out the two periods\n",
    "\n",
    "gmta_first = gmta['1901':'1930']\n",
    "#print(gmta_first)\n",
    "\n",
    "gmta_last = gmta[-30:]\n",
    "#print(len(gmta_last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate th edifference in mean temperature between the two periods\n",
    "\n",
    "diff = np.mean(gmta_last) - np.mean(gmta_first)\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then perform a hypothesis test for difference in the mean (e.g. Student's t test)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "stats.ttest_ind(gmta_last, gmta_first, equal_var = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%, \n",
    "#  then we reject the null hypothesis of equal averages.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
